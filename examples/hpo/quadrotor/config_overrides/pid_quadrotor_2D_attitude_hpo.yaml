hpo_config:

  load_if_exists: True # this should set to True if hpo is run in parallel
  objective: [exponentiated_rmse, exponentiated_rms_action_change] # [other metrics defined in base_experiment.py]
  objective_bounds: [[0.0, 1.0], [0.0, 1.0]] # [bounds for each objective]. Worse value will be assigned if objective evaluation is None
  direction: [maximize, maximize] # [minimize, maximize]
  repetitions: 5 # number of samples of performance for each objective query
  n_episodes: 5 # number of episodes to evaluate each policy
  use_gpu: True
  seed: 24
  save_n_best_hps: 1
  # budget
  trials: 40

  # hyperparameters
  hps_config:
    p_coeff_for: [.4, .4, 1.25]
    i_coeff_for: [.05, .05, .05]
    d_coeff_for: [.2, .2, .5]

    p_coeff_tor: [70000., 70000., 60000.]
    i_coeff_tor: [.0, .0, 500.]
    d_coeff_tor: [20000., 20000., 12000.]
  vizier_hps:
    d_coeff_for:
    - 0.1890092870696659
    - 1.0541122664300269
    - 1.1247264394469974
    d_coeff_tor:
    - 14624.865780764232
    - 14735.949237321409
    - 12928.8956036557
    i_coeff_for:
    - 0.001
    - 0.4045145572372126
    - 0.6961573899435679
    i_coeff_tor:
    - 0.0
    - 7.906897732216709
    - 524.6539429699372
    p_coeff_for:
    - 0.4458780003496436
    - 2.118873126448762
    - 1.2386787893322404
    p_coeff_tor:
    - 64299.26234435173
    - 73880.50880762804
    - 59700.826209375475
  optuna_hps:
    p_coeff_for: [.4, .4, 1.25]
    i_coeff_for: [.05, .05, .05]
    d_coeff_for: [.2, .2, .5]

    p_coeff_tor: [70000., 70000., 60000.]
    i_coeff_tor: [.0, .0, 500.]
    d_coeff_tor: [20000., 20000., 12000.]
