hpo_config:

  load_if_exists: True # this should set to True if hpo is run in parallel
  objective: [exponentiated_rmse, exponentiated_rms_action_change] # [other metrics defined in base_experiment.py]
  objective_bounds: [[0.0, 1.0], [0.0, 1.0]] # [bounds for each objective]. Worse value will be assigned if objective evaluation is None
  direction: [maximize, maximize] # [minimize, maximize]
  repetitions: 5 # number of samples of performance for each objective query
  n_episodes: 5 # number of episodes to evaluate each policy
  use_gpu: True
  seed: 24
  save_n_best_hps: 1
  # budget
  trials: 60

  # hyperparameters
  hps_config:
    # model args
    hidden_dim: 128
    activation: relu

    # loss args
    gamma: 0.99
    gae_lambda: 0.9
    clip_param: 0.2
    target_kl: 0.006
    entropy_coef: 0.02
    quantile_count: 256

    # optim args
    opt_epochs: 20
    mini_batch_size: 256
    actor_lr: 0.0001
    critic_lr: 0.002

    # runner args
    max_env_steps: 660000
    rollout_batch_size: 8

    # objective
    rew_state_weight: [8.0, 0.1, 8.0, 0.1, 0.01, 0.01]
    rew_act_weight: [0.08, 0.01]
