hpo_config:

  load_if_exists: True # this should set to True if hpo is run in parallel
  objective: [exponentiated_rmse, exponentiated_rms_action_change] # [other metrics defined in base_experiment.py]
  objective_bounds: [[0.0, 1.0], [0.0, 1.0]] # [bounds for each objective]. Worse value will be assigned if objective evaluation is None
  direction: [maximize, maximize] # [minimize, maximize]
  repetitions: 5 # number of samples of performance for each objective query
  n_episodes: 5 # number of episodes to evaluate each policy
  use_gpu: True
  seed: 24
  save_n_best_hps: 1
  # budget
  trials: 60

  # hyperparameters
  hps_config:
    # model args
    hidden_dim: 64
    activation: relu

    # loss args
    gamma: 0.98
    gae_lambda: 0.92
    clip_param: 0.2
    target_kl: 0.01
    entropy_coef: 0.005

    # optim args
    opt_epochs: 20
    mini_batch_size: 256
    actor_lr: 0.001
    critic_lr: 0.001

    # runner args
    max_env_steps: 336000
    rollout_batch_size: 5

    # objective
    rew_state_weight: [10.0, 0.1, 10.0, 0.1, 0.1, 0.001]
    rew_act_weight: [0.1, 0.1]
  vizier_hps:
    activation: relu
    actor_lr: 0.0008983071385131106
    clip_param: 0.2
    critic_lr: 0.0010604750567992504
    entropy_coef: 0.027168142112125156
    gae_lambda: 0.92
    gamma: 0.98
    hidden_dim: 64
    max_env_steps: 336000
    mini_batch_size: 256
    opt_epochs: 20
    rew_act_weight:
    - 0.004342434109985312
    - 0.16308846035025895
    rew_state_weight:
    - 8.681025660217175
    - 0.007712483036226952
    - 8.684351881484186
    - 0.25416422315600956
    - 0.2089113565062738
    - 0.042781113613173966
    rollout_batch_size: 5
    target_kl: 0.003516987203810008

