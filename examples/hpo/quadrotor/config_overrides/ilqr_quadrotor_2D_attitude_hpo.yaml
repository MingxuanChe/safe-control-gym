hpo_config:

  load_if_exists: True # this should set to True if hpo is run in parallel
  objective: [exponentiated_rmse, exponentiated_rms_action_change] # [other metrics defined in base_experiment.py]
  objective_bounds: [[0.0, 1.0], [0.0, 1.0]] # [bounds for each objective]. Worse value will be assigned if objective evaluation is None
  direction: [maximize, maximize] # [minimize, maximize]
  repetitions: 5 # number of samples of performance for each objective query
  n_episodes: 5 # number of episodes to evaluate each policy
  use_gpu: True
  seed: 24
  save_n_best_hps: 1
  # budget
  trials: 40

  # hyperparameters
  hps_config:
    max_iterations: 15
    lamb_factor: 10
    lamb_max: 1000
    epsilon: 0.01
    q_lqr: [5, 0.1, 5, 0.1, 0.5, 0.01]
    r_lqr: [1.0, 1.0]
  vizier_hps:
    epsilon: 0.01
    lamb_factor: 10
    lamb_max: 1000
    max_iterations: 15
    q_lqr:
    - 4.903486852466305
    - 0.07811157366922913
    - 5.029240592694074
    - 0.09280561797533883
    - 0.4446202387745285
    - 0.0035923202561466408
    r_lqr:
    - 1.0422280442275553
    - 0.7807458088068064
  optuna_hps:
    epsilon: 0.01
    lamb_factor: 10
    lamb_max: 1000
    max_iterations: 15
    q_lqr:
    - 5.0
    - 0.1
    - 5.0
    - 0.1
    - 0.5
    - 0.01
    r_lqr:
    - 1.0
    - 1.0
