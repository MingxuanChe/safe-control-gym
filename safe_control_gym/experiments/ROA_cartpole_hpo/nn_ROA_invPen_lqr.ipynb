{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tumeilsy-mo-10/anaconda3/envs/safe/lib/python3.10/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# from safe_control_gym.envs.benchmark_env import Task\n",
    "# from safe_control_gym.experiments.base_experiment import BaseExperiment\n",
    "# from safe_control_gym.utils.configuration import ConfigFactory\n",
    "# from safe_control_gym.utils.registration import make\n",
    "\n",
    "\n",
    "from safe_control_gym.experiments.ROA_cartpole.utilities import *\n",
    "from lyapnov import LyapunovNN, Lyapunov, QuadraticFunction, GridWorld_pendulum\n",
    "from utilities import balanced_class_weights, dlqr, \\\n",
    "                      get_discrete_linear_system_matrices, onestep_dynamics\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize) # np print full array\n",
    "\n",
    "class Options(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Options, self).__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "OPTIONS = Options(np_dtype              = np.float32,\n",
    "                  torch_dtype           = torch.float32,\n",
    "                  eps                   = 1e-8,                            # numerical tolerance\n",
    "                  saturate              = True,                            # apply saturation constraints to the control input\n",
    "                  use_zero_threshold    = True,                            # assume the discretization is infinitely fine (i.e., tau = 0)\n",
    "                  pre_train             = True,                            # pre-train the neural network to match a given candidate in a supervised approach\n",
    "                  dpi                   = 150,\n",
    "                  num_cores             = 4,\n",
    "                  num_sockets           = 1,\n",
    "                #   tf_checkpoint_path    = \"./tmp/lyapunov_function_learning.ckpt\"\n",
    "                )\n",
    "\n",
    "# detect torch device\n",
    "myDevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myDevice = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dt = 0.01   # sampling time\n",
    "g = 9.81    # gravity\n",
    "\n",
    "# True system parameters\n",
    "m = 0.15    # pendulum mass\n",
    "L = 0.5     # pole length\n",
    "b = 0.1     # rotational friction\n",
    "\n",
    "# State and action normalizers\n",
    "theta_max = np.deg2rad(180)                     # angular position [rad]\n",
    "omega_max = np.deg2rad(360)                     # angular velocity [rad/s]\n",
    "# u_max     = g * m * L * np.sin(np.deg2rad(60))  # torque [N.m], control action\n",
    "u_max = 0.5\n",
    "\n",
    "state_norm = (theta_max, omega_max)\n",
    "action_norm = (u_max,)\n",
    "\n",
    "# Dimensions and domains\n",
    "state_dim     = 2\n",
    "action_dim    = 1\n",
    "state_limits  = np.array([[-1., 1.]] * state_dim)\n",
    "action_limits = np.array([[-1., 1.]] * action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize system class and its linearization\n",
    "pendulum = InvertedPendulum(m, L, b, dt, [state_norm, action_norm])\n",
    "A, B = pendulum.linearize()\n",
    "# print(\"A\\n \", A)\n",
    "# print(\"B\\n \", B)\n",
    "# dynamics = pendulum.__call__\n",
    "dynamics = pendulum.__call__\n",
    "\n",
    "# # test dynamics with state [0.5, 0] and action [0.5]\n",
    "# x = torch.tensor([0.5, 0.], dtype=OPTIONS.torch_dtype)\n",
    "# u = torch.tensor([-0.5], dtype=OPTIONS.torch_dtype)\n",
    "# x = np.array([0.5, 0.])\n",
    "# u = np.array([-0.5])\n",
    "# print(\"x: \", x)\n",
    "# print(\"u: \", u)\n",
    "# state_action = np.concatenate([x, u])\n",
    "# state_action = torch.cat([x, u])\n",
    "# print(\"state_action: \", state_action)\n",
    "\n",
    "# x_next = dynamics(state_action)\n",
    "# print(\"x_next: \", x_next)\n",
    "# assert x_next.shape == x.shape\n",
    "# print(\"Dynamics shape test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_constraints:  [[-3.14159265  3.14159265]\n",
      " [-6.28318531  6.28318531]]\n",
      "state_discretization.all_points.shape:  (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "state_constraints = np.array([[-theta_max, theta_max], [-omega_max, omega_max]])\n",
    "\n",
    "print('state_constraints: ', state_constraints)\n",
    "num_states = 100\n",
    "\n",
    "grid_limits = np.array([[-1., 1.], ] * state_dim)\n",
    "# state_discretization = gridding(state_dim, state_constraints=None, num_states = 100)\n",
    "state_discretization = GridWorld_pendulum(grid_limits, num_states)\n",
    "# state_discretization = gridding(state_dim, state_constraints, num_states = 100)\n",
    "print('state_discretization.all_points.shape: ', state_discretization.all_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: 10000\n",
      "Discretization constant (tau): 0.0\n",
      "state_discretization.all_points.shape:  (10000, 2)\n",
      "initial_safe_set.sum():  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Discretization constant\n",
    "if OPTIONS.use_zero_threshold:\n",
    "    tau = 0.0\n",
    "else:\n",
    "    tau = np.sum(state_discretization.unit_maxes) / 2\n",
    "\n",
    "print('Grid size: {}'.format(state_discretization.nindex))\n",
    "print('Discretization constant (tau): {}'.format(tau))\n",
    "\n",
    "# Set initial safe set as a ball around the origin (in normalized coordinates)\n",
    "cutoff_radius    = 0.1\n",
    "initial_safe_set = np.linalg.norm(state_discretization.all_points, ord=2, axis=1) <= cutoff_radius\n",
    "print('state_discretization.all_points.shape: ', state_discretization.all_points.shape)\n",
    "print('initial_safe_set.sum(): ', initial_safe_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [ 0.19997438 -0.00283275]\n",
      "x:  [ 0.19989275 -0.00559486]\n",
      "x:  [ 0.19975647 -0.00829254]\n",
      "x:  [ 0.19956678 -0.01093174]\n",
      "x:  [ 0.19932478 -0.01351815]\n",
      "x:  [ 0.19903149 -0.01605725]\n",
      "x:  [ 0.1986878  -0.01855429]\n",
      "x:  [ 0.19829452 -0.02101433]\n",
      "x:  [ 0.19785233 -0.02344227]\n",
      "x:  [ 0.19736183 -0.0258428 ]\n",
      "x:  [ 0.19682354 -0.02822047]\n",
      "x:  [ 0.19623787 -0.03057971]\n",
      "x:  [ 0.19560515 -0.03292479]\n",
      "x:  [ 0.19492562 -0.03525986]\n",
      "x:  [ 0.19419945 -0.03758898]\n",
      "x:  [ 0.19342672 -0.03991608]\n",
      "x:  [ 0.19260745 -0.04224503]\n",
      "x:  [ 0.19174154 -0.04457961]\n",
      "x:  [ 0.19082887 -0.04692351]\n",
      "x:  [ 0.18986921 -0.04928039]\n",
      "x:  [ 0.18886227 -0.05165383]\n",
      "x:  [ 0.18780769 -0.05404738]\n",
      "x:  [ 0.18670502 -0.05646454]\n",
      "x:  [ 0.18555378 -0.05890879]\n",
      "x:  [ 0.18435338 -0.06138358]\n",
      "x:  [ 0.18310319 -0.06389236]\n",
      "x:  [ 0.18180249 -0.06643856]\n",
      "x:  [ 0.1804505  -0.06902561]\n",
      "x:  [ 0.17904638 -0.07165695]\n",
      "x:  [ 0.17758921 -0.07433604]\n",
      "x:  [ 0.176078   -0.07706636]\n",
      "x:  [ 0.1745117 -0.0798514]\n",
      "x:  [ 0.17288918 -0.08269471]\n",
      "x:  [ 0.17120924 -0.08559988]\n",
      "x:  [ 0.16947062 -0.08857052]\n",
      "x:  [ 0.16767196 -0.09161033]\n",
      "x:  [ 0.16581186 -0.09472305]\n",
      "x:  [ 0.16388883 -0.0979125 ]\n",
      "x:  [ 0.16190128 -0.10118258]\n",
      "x:  [ 0.15984758 -0.10453725]\n",
      "x:  [ 0.15772599 -0.1079806 ]\n",
      "x:  [ 0.15553471 -0.11151678]\n",
      "x:  [ 0.15327183 -0.11515006]\n",
      "x:  [ 0.15093539 -0.11888482]\n",
      "x:  [ 0.1485233  -0.12272556]\n",
      "x:  [ 0.14603341 -0.12667691]\n",
      "x:  [ 0.14346611 -0.13045082]\n",
      "x:  [ 0.14082919 -0.1335784 ]\n",
      "x:  [ 0.13813509 -0.13611338]\n",
      "x:  [ 0.13539521 -0.13810588]\n",
      "x:  [ 0.13261996 -0.13960266]\n",
      "x:  [ 0.12981887 -0.14064726]\n",
      "x:  [ 0.12700062 -0.1412802 ]\n",
      "x:  [ 0.1241731  -0.14153918]\n",
      "x:  [ 0.12134346 -0.14145922]\n",
      "x:  [ 0.1185182  -0.14107285]\n",
      "x:  [ 0.11570316 -0.14041021]\n",
      "x:  [ 0.11290361 -0.13949925]\n",
      "x:  [ 0.1101243  -0.13836584]\n",
      "x:  [ 0.10736945 -0.13703389]\n",
      "x:  [ 0.10464283 -0.1355255 ]\n",
      "x:  [ 0.10194779 -0.13386108]\n",
      "x:  [ 0.09928727 -0.13205942]\n",
      "x:  [ 0.09666386 -0.13013784]\n",
      "x:  [ 0.09407983 -0.12811225]\n",
      "x:  [ 0.0915371 -0.1259973]\n",
      "x:  [ 0.08903736 -0.1238064 ]\n",
      "x:  [ 0.086582   -0.12155186]\n",
      "x:  [ 0.08417221 -0.11924493]\n",
      "x:  [ 0.08180892 -0.11689589]\n",
      "x:  [ 0.07949291 -0.11451413]\n",
      "x:  [ 0.07722474 -0.11210818]\n",
      "x:  [ 0.07500483 -0.10968583]\n",
      "x:  [ 0.07283345 -0.10725411]\n",
      "x:  [ 0.07071072 -0.10481941]\n",
      "x:  [ 0.06863666 -0.1023875 ]\n",
      "x:  [ 0.06661115 -0.09996357]\n",
      "x:  [ 0.064634   -0.09755229]\n",
      "x:  [ 0.06270491 -0.09515785]\n",
      "x:  [ 0.06082353 -0.09278398]\n",
      "x:  [ 0.05898939 -0.09043401]\n",
      "x:  [ 0.05720201 -0.08811088]\n",
      "x:  [ 0.05546081 -0.08581718]\n",
      "x:  [ 0.05376519 -0.08355519]\n",
      "x:  [ 0.05211451 -0.08132688]\n",
      "x:  [ 0.05050806 -0.07913397]\n",
      "x:  [ 0.04894513 -0.07697791]\n",
      "x:  [ 0.04742497 -0.07485993]\n",
      "x:  [ 0.04594681 -0.07278107]\n",
      "x:  [ 0.04450986 -0.07074215]\n",
      "x:  [ 0.04311331 -0.06874385]\n",
      "x:  [ 0.04175635 -0.06678667]\n",
      "x:  [ 0.04043816 -0.06487098]\n",
      "x:  [ 0.03915789 -0.06299701]\n",
      "x:  [ 0.03791472 -0.06116488]\n",
      "x:  [ 0.03670781 -0.05937461]\n",
      "x:  [ 0.03553632 -0.05762611]\n",
      "x:  [ 0.03439942 -0.05591921]\n",
      "x:  [ 0.03329627 -0.05425367]\n",
      "x:  [ 0.03222606 -0.05262918]\n"
     ]
    }
   ],
   "source": [
    "Q = np.identity(state_dim).astype(OPTIONS.np_dtype)     # state cost matrix\n",
    "Q = np.diag([5, 1])\n",
    "R = 1* np.identity(action_dim).astype(OPTIONS.np_dtype)    # action cost matrix\n",
    "# K, P_lqr = safe_learning.utilities.dlqr(A, B, Q, R)\n",
    "K, P_lqr = dlqr(A, B, Q, R) \n",
    "# print('K',K)\n",
    "\n",
    "policy = lambda x: -K @ x\n",
    "if OPTIONS.saturate:\n",
    "    # policy = lambda x: np.clip(-K @ x, -u_max, u_max)\n",
    "    policy = lambda x: np.clip(-K @ x, -1, 1)\n",
    "\n",
    "cl_dynamics = lambda x: dynamics(np.concatenate([x, policy(x)]))\n",
    "\n",
    "# test the closed-loop dynamics with state [0.5, 0]\n",
    "x = np.array([0.20, 0.])\n",
    "for i in range(100):\n",
    "    x = cl_dynamics(x)\n",
    "    print(\"x: \", x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_pol = lambda x: np.linalg.norm(-K, 1)\n",
    "\n",
    "L_dyn = lambda x: np.linalg.norm(A, 1) + np.linalg.norm(B, 1) * L_pol(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lyapunov_lqr.safe_set.sum()\n",
      " 2006\n"
     ]
    }
   ],
   "source": [
    "lyapunov_function = QuadraticFunction(P_lqr)\n",
    "\n",
    "# Approximate local Lipschitz constants with gradients\n",
    "# grad_lyapunov_function = lambda x: 2 * P_lqr @ x\n",
    "grad_lyapunov_function = lambda x: 2 * torch.tensor(P_lqr, dtype=torch.float32) @ x\n",
    "# L_v = lambda x: tf.norm(grad_lyapunov_function(x), ord=1, axis=1, keepdims=True)\n",
    "L_v = lambda x: torch.norm(grad_lyapunov_function(x), p=1, dim=-1, keepdim=True)\n",
    "\n",
    "# Initialize Lyapunov class\n",
    "lyapunov_lqr = Lyapunov(state_discretization, lyapunov_function, cl_dynamics, L_dyn, L_v, tau, policy, initial_safe_set)\n",
    "# print the first 10 states in the safe set\n",
    "lyapunov_lqr.discretization.all_points[1:10]\n",
    "# print('lyaupunov_lqr.discretization.all_points[1:10]\\n', lyapunov_lqr.discretization.all_points[1:10])\n",
    "lyapunov_lqr.update_values()\n",
    "lyapunov_lqr.update_safe_set()\n",
    "# print('lyapunov_lqr.safe_set\\n', lyapunov_lqr.safe_set[1:10])\n",
    "# print('lyapunov_lqr.values\\n', lyapunov_lqr.values[1:10])\n",
    "# print('lyapunov_lqr.c_max\\n', lyapunov_lqr.c_max)\n",
    "print('lyapunov_lqr.safe_set.sum()\\n', lyapunov_lqr.safe_set.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ROA size:2518\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "horizon = 500\n",
    "tol = 0.1\n",
    "# roa, trajectories = compute_roa_pendulum(lyapunov_lqr.discretization, cl_dynamics, horizon, tol, no_traj=False)\n",
    "compute_new_roa = False\n",
    "# compute_new_roa = True\n",
    "\n",
    "# get the current path of this jupyter notebook\n",
    "script_dir = os.getcwd()\n",
    "roa_file_name = 'roa_pendulum.npy'\n",
    "traj_file_name = 'traj_pendulum.npy'\n",
    "# append the file name to the current path\n",
    "roa_file_name = os.path.join(script_dir, roa_file_name)\n",
    "traj_file_name = os.path.join(script_dir, traj_file_name)\n",
    "if not compute_new_roa:\n",
    "    # load the pre-saved ROA to avoid re-computation\n",
    "    roa = np.load(roa_file_name)\n",
    "    trajectories = np.load(traj_file_name)\n",
    "else:\n",
    "    roa, trajectories = compute_roa_pendulum(lyapunov_lqr.discretization, cl_dynamics, horizon, tol, no_traj=False)\n",
    "    np.save(roa_file_name, roa)\n",
    "    np.save(traj_file_name, trajectories)\n",
    "    # exit()\n",
    "\n",
    "print('True ROA size:{}\\n'.format(int(roa.sum())))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn:  LyapunovNN(\n",
      "  (layers): ModuleList()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "######################## define Lyapunov NN ########################\n",
    "# initialize Lyapunov NN\n",
    "from lyapnov import LyapunovNN\n",
    "\n",
    "layer_dim = [64, 64, 64]\n",
    "# layer_dim = [128, 128, 128]\n",
    "activations = [torch.nn.Tanh(), torch.nn.Tanh(), torch.nn.Tanh()]\n",
    "nn = LyapunovNN(state_dim, layer_dim, activations)\n",
    "print('nn: ', nn)\n",
    "\n",
    "for name, param in nn.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "\n",
    "# forward some random states\n",
    "x = torch.randn(state_dim,)\n",
    "# print('x: ', x)\n",
    "# print('nn(x): ', nn(x))\n",
    "\n",
    "for name, param in nn.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximate local Lipschitz constant with gradient\n",
    "grad_lyapunov_function = \\\n",
    "    lambda x: torch.autograd.grad(nn(x), x, \\\n",
    "                    torch.ones_like(nn(x)), allow_unused=True,)[0]\n",
    "lyapunov_nn = Lyapunov(state_discretization, nn, \\\n",
    "                          cl_dynamics, L_dyn, L_v, tau, policy, \\\n",
    "                          initial_safe_set)\n",
    "lyapunov_nn.update_values()\n",
    "lyapunov_nn.update_safe_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# train the parameteric Lyapunov candidate in order to expand the verifiable\n",
    "# safe set toward the brute-force safe set\n",
    "test_classfier_loss = []\n",
    "test_decrease_loss   = []\n",
    "roa_estimate         = np.copy(lyapunov_nn.safe_set)\n",
    "\n",
    "# grid              = lyapunov_lqr.discretization\n",
    "grid              = lyapunov_nn.discretization\n",
    "c_max             = [lyapunov_nn.c_max, ]\n",
    "safe_set_fraction = [lyapunov_nn.safe_set.sum() / grid.nindex, ]\n",
    "print('safe_set_fraction', safe_set_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### traning hyperparameters #######################\n",
    "outer_iters = 5\n",
    "inner_iters = 10\n",
    "horizon     = 100\n",
    "test_size   = int(1e4)\n",
    "\n",
    "safe_level = 1\n",
    "lagrange_multiplier = 5000\n",
    "level_multiplier = 1.3\n",
    "learning_rate = 5e-3\n",
    "batch_size    = int(1e3)\n",
    "\n",
    "optimizer = torch.optim.SGD(lyapunov_nn.lyapunov_function.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# training loop #############################\n",
    "print('Current metrics ...')\n",
    "c = lyapunov_nn.c_max\n",
    "num_safe = lyapunov_nn.safe_set.sum()\n",
    "print('Safe level (c_k): {}'.format(c))\n",
    "print('Safe set size: {} ({:.2f}% of grid, \\\n",
    "        {:.2f}% of ROA)\\n'.format(int(num_safe), \\\n",
    "        100 * num_safe / grid.nindex, 100 * num_safe / roa.sum()))\n",
    "print('')\n",
    "time.sleep(0.5)\n",
    "for _ in range(outer_iters):\n",
    "    print('Iteration (k): {}'.format(len(c_max)))\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    ## Identify the \"gap\" states, i.e., those between V(c_k) \n",
    "    ## and V(a * c_k) for a > 1\n",
    "    c = lyapunov_nn.c_max\n",
    "    idx_small = lyapunov_nn.values.ravel() <= c\n",
    "    # print('lyapunov_nn.values.ravel()', lyapunov_nn.values.ravel())\n",
    "    # print('c', c)\n",
    "    # print('level_multiplier', level_multiplier)\n",
    "    idx_big   = lyapunov_nn.values.ravel() <= level_multiplier * c\n",
    "    # print('idx_small', idx_small)\n",
    "    # print('idx_big', idx_big)\n",
    "    idx_gap   = np.logical_and(idx_big, ~idx_small)\n",
    "\n",
    "    ## Forward-simulate \"gap\" states to determine \n",
    "    ## which ones we can add to our ROA estimate\n",
    "    gap_states = grid.all_points[idx_gap]\n",
    "    print('gap_states\\n', gap_states)\n",
    "    print('gap_states.shape', gap_states.shape)\n",
    "    # input('press enter to continue')\n",
    "    gap_future_values = np.zeros((gap_states.shape[0], 1))\n",
    "    for state_idx in range(gap_states.shape[0]):\n",
    "        # !! when using dynamics, the state can go out of the bound\n",
    "        for _ in range(horizon):\n",
    "            # print('gap_states[state_idx]', gap_states[state_idx])\n",
    "            # print('gap_states[state_idx].shape', gap_states[state_idx].shape)\n",
    "            # # print('policy(gap_states[state_idx])', policy(gap_states[state_idx]))\n",
    "            # print('dynamics(gap_states[state_idx])', \\\n",
    "            #         dynamics(gap_states[state_idx]))\n",
    "            # print('dynamics(gap_states[state_idx]).shape', \\\n",
    "            #         dynamics(gap_states[state_idx]).shape)\n",
    "            # dynamics return the next state in the form of (4, 1)\n",
    "            # to feed into gap_states[state_idx], we need to reshape it to (4,)\n",
    "            gap_states[state_idx] = np.reshape(cl_dynamics(gap_states[state_idx]), -1)\n",
    "        # use the safe-control-gym to simulate the trajectory\n",
    "        # init_state = gap_states[state_idx]\n",
    "        # init_state_dict = {'init_x': init_state[0], 'init_x_dot': init_state[1], \\\n",
    "                            # 'init_theta': init_state[2], 'init_theta_dot': init_state[3]}\n",
    "        # init_state, _ = random_env.reset(init_state = init_state_dict)\n",
    "        # print('init_state', init_state)\n",
    "        # static_env = env_func(gui=False, random_state=False, init_state=init_state)\n",
    "        # static_train_env = env_func(gui=False, randomized_init=False, init_state=init_state)\n",
    "        # Create experiment, train, and run evaluation\n",
    "        # experiment = BaseExperiment(env=static_env, ctrl=ctrl, train_env=static_train_env)\n",
    "        # simulate the gap state in safe-control-gym for only pre-defined horizon\n",
    "        # trajs_data, _ = experiment.run_evaluation(training=True, n_steps=horizon, verbose=False)\n",
    "        # print('trajectory data\\n', trajs_data)\n",
    "        # print('trajs_data\\n', trajs_data['info'][0][-1])\n",
    "        # print('obs[0]\\n', trajs_data['obs'][0][-1])\n",
    "        # gap_states[state_idx] = trajs_data['obs'][0][-1]\n",
    "        # print('gap_states[state_idx]', gap_states[state_idx])\n",
    "        # print('gap_states[state_idx].shape', gap_states[state_idx].shape)\n",
    "        # print('gap_states[state_idx] type', type(gap_states[state_idx]))\n",
    "        gap_future_values[state_idx] = (lyapunov_nn.lyapunov_function(\\\n",
    "                                    gap_states[state_idx])).detach().numpy()\n",
    "        # exit()\n",
    "        # Close environments\n",
    "        # static_env.close()\n",
    "        # static_train_env.close()\n",
    "    # print('gap_states\\n', gap_states)\n",
    "    # print('gap_future_values\\n', gap_future_values)\n",
    "    # reshape c to match the dim of gap_future_values\n",
    "    c_vec = np.ones_like(gap_future_values) * c\n",
    "    # concatenate all gap states with gap future values as a sanity check\n",
    "    # also concatenate current safe set with current safe level\n",
    "    res = np.hstack((c_vec, gap_future_values, gap_states))\n",
    "    print('gap state and future values res\\n', res)\n",
    "    print('\\n')\n",
    "    # print('roa_estimate[idx_gap] before ior', roa_estimate[idx_gap])\n",
    "    # print('roa_estimate[idx_gap] shape', roa_estimate[idx_gap].shape)\n",
    "    roa_estimate[idx_gap] |= (gap_future_values <= c).ravel()\n",
    "    # print('roa_estimate[idx_gap] after ior', roa_estimate[idx_gap])\n",
    "    print('roa_estimate[idx_gap] shape', roa_estimate[idx_gap].shape)\n",
    "    # input('press enter to continue')\n",
    "\n",
    "    ## Identify the class labels for our current ROA estimate \n",
    "    ## and the expanded level set\n",
    "    target_idx = np.logical_or(idx_big, roa_estimate)\n",
    "    target_set = grid.all_points[target_idx]\n",
    "    target_labels = roa_estimate[target_idx]\\\n",
    "                    .astype(OPTIONS.np_dtype).reshape([-1, 1])\n",
    "    print('target_labels\\n', target_labels.T)\n",
    "    idx_range = target_set.shape[0]\n",
    "    # print('idx_range', idx_range)\n",
    "    # exit()\n",
    "\n",
    "    ## test set\n",
    "    # idx_batch = tf.random_uniform([batch_size, ], 0, idx_range, dtype=tf.int32, name='batch_sample')\n",
    "    idx_test = np.random.randint(0, idx_range, size=(test_size, ))\n",
    "    test_set = target_set[idx_test]\n",
    "    test_labels = target_labels[idx_test]\n",
    "\n",
    "    # stochastic gradient descent for classification\n",
    "    for _ in range(inner_iters):\n",
    "        lyapunov_nn.lyapunov_function.train()\n",
    "        # training step\n",
    "        # safe_level = lyapunov_nn.c_max\n",
    "        idx_batch_eval = np.random.randint(0, idx_range, size=(batch_size, ))\n",
    "        training_states = target_set[idx_batch_eval]\n",
    "        num_training_states = training_states.shape[0]\n",
    "        print('training_states\\n', training_states)\n",
    "        \n",
    "        # True class labels, converted from Boolean ROA labels {0, 1} to {-1, 1}\n",
    "        roa_labels = target_labels[idx_batch_eval]\n",
    "        class_label = 2 * roa_labels - 1\n",
    "        class_label = torch.tensor(class_label, dtype=torch.float32, device=myDevice)\n",
    "        print('roa_labels\\n', roa_labels.T)\n",
    "\n",
    "        # concatenate all training states with class labels as a sanity check\n",
    "        res = np.hstack((class_label, training_states))\n",
    "        print('training states and class labels res\\n', res)\n",
    "        print('\\n')\n",
    "        # exit()\n",
    "        # Signed, possibly normalized distance from the decision boundary\n",
    "        # print('safe_level\\n', safe_level)\n",
    "        decision_distance_for_states = torch.zeros((num_training_states, 1), dtype=torch.float32, device=myDevice)                                                   \n",
    "        for state_idx in range(num_training_states):\n",
    "            # decision_distance_for_states[state_idx] = safe_level - lyapunov_nn.lyapunov_function(training_states[state_idx].reshape(-1, 1))\n",
    "            decision_distance_for_states[state_idx] = lyapunov_nn.lyapunov_function(training_states[state_idx])\n",
    "        \n",
    "        decision_distance = safe_level - decision_distance_for_states\n",
    "        print('decision_distance\\n', decision_distance.T)\n",
    "        # exit()\n",
    "        \n",
    "        # Perceptron loss with class weights\n",
    "        class_weights, class_counts = balanced_class_weights(roa_labels.astype(bool))\n",
    "        # print('class_weights\\n', class_weights.T)\n",
    "        # classifier_loss = class_weights * tf.maximum(- class_labels * decision_distance, 0, name='classifier_loss')\n",
    "        # print('class_weights\\n', class_weights)\n",
    "        # print('class_weights type\\n', type(class_weights))\n",
    "        # convert class_weights to torch tensor\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float32, device=myDevice)\n",
    "        classifier_loss = class_weights * torch.max(- class_label * decision_distance, torch.zeros_like(decision_distance, device=myDevice)) \n",
    "        # print('classifier_loss\\n', classifier_loss.T)\n",
    "        # input('press enter to continue')\n",
    "        # exit()\n",
    "        # Enforce decrease constraint with Lagrangian relaxation\n",
    "        # decrease_loss = roa_labels * tf.maximum(tf_dv_nn, 0) / tf.stop_gradient(tf_values_nn + OPTIONS.eps)\n",
    "        torch_dv_nn = torch.zeros((num_training_states, 1), dtype=torch.float32, device=myDevice)\n",
    "        for state_idx in range(num_training_states):\n",
    "            future_state = np.reshape(cl_dynamics(training_states[state_idx]), -1)\n",
    "            torch_dv_nn[state_idx] = lyapunov_nn.lyapunov_function(\\\n",
    "                                future_state) - \\\n",
    "                                lyapunov_nn.lyapunov_function(training_states[state_idx])\n",
    "        print('torch_dv_nn\\n', torch_dv_nn.T)\n",
    "        # exit()\n",
    "        roa_labels = torch.tensor(roa_labels, dtype=torch.float32, device=myDevice).detach()\n",
    "        training_states_forwards = torch.zeros((num_training_states, 1), dtype=torch.float32, device=myDevice).detach()\n",
    "        for state_idx in range(num_training_states):\n",
    "            training_states_forwards[state_idx] = lyapunov_nn.lyapunov_function(training_states[state_idx])\n",
    "        # TODO: check ROA labels\n",
    "        print('torch.max(torch_dv_nn, torch.zeros_like(torch_dv_nn))\\n', torch.max(torch_dv_nn, torch.zeros_like(torch_dv_nn)).T)\n",
    "        decrease_loss = roa_labels * torch.max(torch_dv_nn, torch.zeros_like(torch_dv_nn))  \\\n",
    "                            # /(training_states_forwards + OPTIONS.eps)\n",
    "        print('decrease_loss\\n', decrease_loss.T)\n",
    "        loss = torch.mean(classifier_loss + lagrange_multiplier * decrease_loss)\n",
    "        print('loss\\n', loss)\n",
    "        # input('press enter to continue')\n",
    "        optimizer.zero_grad() # zero gradiants for every batch !!\n",
    "        loss.backward()\n",
    "        # print('optimizer\\n', optimizer)\n",
    "        # exit()\n",
    "        optimizer.step()\n",
    "        # exit()\n",
    "    \n",
    "    ## Update Lyapunov values and ROA estimate, \n",
    "    ## based on new parameter values\n",
    "    print('lyapunov_nn.values before update\\n', lyapunov_nn.values[1:10])\n",
    "    lyapunov_nn.update_values()  \n",
    "    print('lyapunov_nn.values\\n', lyapunov_nn.values[1:10])\n",
    "    lyapunov_nn.update_safe_set()\n",
    "    roa_estimate |= lyapunov_nn.safe_set\n",
    "\n",
    "    c_max.append(lyapunov_nn.c_max)\n",
    "    safe_set_fraction.append(lyapunov_nn.safe_set.sum() / grid.nindex)\n",
    "    print('Current safe level (c_k): {}'.format(c_max[-1]))\n",
    "    print('Safe set size: {} ({:.2f}% of grid, {:.2f}% of ROA)\\n'.format(\n",
    "                            int(lyapunov_nn.safe_set.sum()), \\\n",
    "                            100 * safe_set_fraction[-1], \\\n",
    "                            100 * safe_set_fraction[-1] * roa.size / roa.sum()\\\n",
    "                                ))\n",
    "    # input('press enter to continue')\n",
    "\n",
    "print('c_max', c_max)\n",
    "print('safe_set_fraction', safe_set_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ plotting ################################\n",
    "fig = plt.figure(figsize=(8, 3), dpi=OPTIONS.dpi, frameon=False)\n",
    "fig.subplots_adjust(wspace=0.35)\n",
    "plot_limits = np.column_stack((- np.rad2deg([theta_max, omega_max]), np.rad2deg([theta_max, omega_max])))\n",
    "\n",
    "# ax = plt.subplot(121)\n",
    "ax = plt.subplot(111)\n",
    "alpha = 1\n",
    "colors = [None] * 4\n",
    "colors[0] = (0, 158/255, 115/255)       # ROA - bluish-green\n",
    "colors[1] = (230/255, 159/255, 0)       # NN  - orange\n",
    "colors[2] = (0, 114/255, 178/255)       # LQR - blue\n",
    "colors[3] = (240/255, 228/255, 66/255)  # SOS - yellow\n",
    "\n",
    "# True ROA\n",
    "z = roa.reshape(grid.num_points)\n",
    "# print(z.shape)\n",
    "# print(z)\n",
    "ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), colors=(colors[0],), linewidths=1)\n",
    "ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), cmap=binary_cmap(colors[0]), alpha=alpha)\n",
    "\n",
    "# # Neural network\n",
    "z = lyapunov_nn.safe_set.reshape(grid.num_points)\n",
    "ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), colors=(colors[1],), linewidths=1)\n",
    "ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), cmap=binary_cmap(colors[1]), alpha=alpha)\n",
    "\n",
    "# LQR\n",
    "z = lyapunov_lqr.safe_set.reshape(grid.num_points)\n",
    "ax.contour(z.T, origin='lower', extent=plot_limits.ravel(), colors=(colors[2],), linewidths=1)\n",
    "ax.imshow(z.T, origin='lower', extent=plot_limits.ravel(), cmap=binary_cmap(colors[2]), alpha=alpha)\n",
    "\n",
    "\n",
    "\n",
    "# Plot some trajectories\n",
    "N_traj = 11\n",
    "skip = int(grid.num_points[0] / N_traj)\n",
    "sub_idx = np.arange(grid.nindex).reshape(grid.num_points)\n",
    "sub_idx = sub_idx[::skip, ::skip].ravel()\n",
    "sub_trajectories = trajectories[sub_idx, :, :]\n",
    "sub_states = grid.all_points[sub_idx]\n",
    "for n in range(sub_trajectories.shape[0]):\n",
    "    x = sub_trajectories[n, 0, :] * np.rad2deg(theta_max)\n",
    "    y = sub_trajectories[n, 1, :] * np.rad2deg(omega_max)\n",
    "    ax.plot(x, y, 'k--', linewidth=0.25)\n",
    "sub_states = grid.all_points[sub_idx]\n",
    "dx_dt = np.zeros_like(sub_states)\n",
    "for state_idx in range(sub_states.shape[0]):\n",
    "    dx_dt[state_idx, :] = (cl_dynamics(sub_states[state_idx, :]) - sub_states[state_idx, :])/dt\n",
    "\n",
    "# dx_dt = (tf_future_states.eval({tf_states: sub_states}) - sub_states) / dt\n",
    "dx_dt = dx_dt / np.linalg.norm(dx_dt, ord=2, axis=1, keepdims=True)\n",
    "ax.quiver(sub_states[:, 0] * np.rad2deg(theta_max), sub_states[:, 1] * np.rad2deg(omega_max), dx_dt[:, 0], dx_dt[:, 1], \n",
    "          scale=None, pivot='mid', headwidth=3, headlength=6, color='k')\n",
    "\n",
    "ax.set_title('ROA of pendulum under an LQR prior policy (l={:.2f})'.format(L))\n",
    "ax.set_aspect(theta_max / omega_max / 1.2)\n",
    "ax.set_xlim(plot_limits[0])\n",
    "ax.set_ylim(plot_limits[1])\n",
    "ax.set_xlabel(r'angle [deg]')\n",
    "ax.set_ylabel(r'angular velocity [deg/s]')\n",
    "ax.xaxis.set_ticks(np.arange(-180, 181, 60))\n",
    "ax.yaxis.set_ticks(np.arange(-360, 361, 120))\n",
    "\n",
    "proxy = [plt.Rectangle((0,0), 1, 1, fc=c) for c in colors]    \n",
    "legend = ax.legend(proxy, [r'Brute-forced ROA'], loc='upper right')\n",
    "legend.get_frame().set_alpha(1.)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
